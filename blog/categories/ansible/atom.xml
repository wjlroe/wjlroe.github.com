<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ansible | William Roe&#146;s blog]]></title>
  <link href="http://blog.wjlr.org.uk/blog/categories/ansible/atom.xml" rel="self"/>
  <link href="http://blog.wjlr.org.uk/"/>
  <updated>2015-01-16T06:14:33+00:00</updated>
  <id>http://blog.wjlr.org.uk/</id>
  <author>
    <name><![CDATA[Will Roe]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Multi-Machine Vagrant Ansible Gotcha]]></title>
    <link href="http://blog.wjlr.org.uk/2014/12/30/multi-machine-vagrant-ansible-gotcha.html"/>
    <updated>2014-12-30T16:00:00+00:00</updated>
    <id>http://blog.wjlr.org.uk/2014/12/30/multi-machine-vagrant-ansible-gotcha</id>
    <content type="html"><![CDATA[<p><a href="https://docs.vagrantup.com/" title="Vagrant">Vagrant</a> is a fantastic tool for defining how virtual instances are
to be run and provisioned. I&#8217;ve used Vagrant with <a href="https://docs.chef.io/chef_solo.html" title="Chef-solo">Chef-Solo</a> and
<a href="http://docs.ansible.com/index.html">Ansible</a> provisioners and it&#8217;s helped me understand those tools and
iterate quickly. There are some gotchas however and in this post I
will explore a particular flaw in the way Vagrant and Ansible
cooperate.</p>

<h2>Multi-machine setup</h2>

<p>Let&#8217;s begin by defining a Vagrant environment that we will play with
(you will need <a href="https://www.virtualbox.org">VirtualBox</a>, Vagrant and Ansible installed):</p>

<pre><code class="bash">mkdir multi-vagrant-ansible
cd multi-vagrant-ansible
vagrant init
</code></pre>

<p>This will create a <code>Vagrantfile</code> in the current directory with
commented contents. Let&#8217;s cut it back to the essentials and add in a
URL for the base box (<a href="https://cloud-images.ubuntu.com/vagrant/" title="Ubuntu Cloud Vagrant images">Ubuntu Trusty</a> is the latest LTS
release so that&#8217;s what I&#8217;ll use):</p>

<pre><code class="ruby"># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure(2) do |config|
  config.vm.box = "trusty64"
  config.vm.box_url = "https://cloud-images.ubuntu.com/vagrant/trusty/current/trusty-server-cloudimg-amd64-vagrant-disk1.box"
  config.vm.network "private_network", type: "dhcp"
end
</code></pre>

<p>If we run Vagrant now, it&#8217;ll clone that base box (downloading it first
if it hasn&#8217;t already done so) and boot it up. This is already quicker
than downloading an ISO, creating a new VirtualBox instance, booting
that up and going through the installation procedure.</p>

<p>Let&#8217;s define some machines and set them up to be provisioned by
Ansible. We&#8217;ll have two web servers and one load balancer, because
that&#8217;s boringly conventional:</p>

<pre><code class="ruby">  config.vm.define "ariadne" do |ariadne|
    ariadne.vm.provision "ansible" do |ansible|
      ansible.playbook = "loadbalancer.yml"
      ansible.sudo = true
    end
  end

  config.vm.define "minos" do |minos|
    minos.vm.provision "ansible" do |ansible|
      ansible.playbook = "webserver.yml"
      ansible.sudo = true
    end
  end

  config.vm.define "pasiphae" do |pasiphae|
    pasiphae.vm.provision "ansible" do |ansible|
      ansible.playbook = "webserver.yml"
      ansible.sudo = true
    end
  end
</code></pre>

<p>So in the above, <code>minos</code> and <code>pasiphae</code> are web servers (i.e. they
will be running <a href="http://nginx.org">nginx</a>) and <code>ariadne</code> is the load balancer. The
location of the ansible playbooks are relative to the Vagrantfile so
in the same directory we will create <code>webserver.yml</code> with the
following contents:</p>

<pre><code class="yaml">---

- hosts: all
  tasks:
    - apt: name=nginx state=present
    - service: name=nginx state=started
</code></pre>

<p>Which ensures that nginx is not only installed but also running (it&#8217;ll
also mean that if that server is rebooted, it&#8217;ll still run nginx).</p>

<p>Now for the load balancer, <code>loadbalancer.yml</code>:</p>

<pre><code class="yaml">---

- hosts: all
  tasks:
    - apt: name=haproxy state=present
    - service: name=haproxy state=started
</code></pre>

<p>Which ensures <a href="http://www.haproxy.org">haproxy</a> is installed and running in the same way.</p>

<p>These two playbooks are not aware of each other, they act
independently and you could use <code>ansible-playbook</code> to provision any
server you liked with them.</p>

<p>If you run <code>vagrant up</code> at this point (assuming you&#8217;ve not done that
with this Vagrantfile before), it&#8217;ll boot up new VirtualBox instances
and provision them with ansible, installing the necessary software
etc. All well and good so far.</p>

<h2>Ansible facts</h2>

<p>Ansible starts off by collecting facts about the nodes it&#8217;ll run on.
It does this so that you can use information about the node in your
playbooks, roles and tasks.</p>

<p>To see the kind of facts that ansible collects about a node, you can
run ansible&#8217;s <code>setup</code> module like this (for the minos instance):</p>

<pre><code class="bash">ansible -i .vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory -m setup -u vagrant --private-key=.vagrant/machines/minos/virtualbox/private_key minos
</code></pre>

<p>The above command should print out a large JSON structure of all the
facts ansible has collected about that node. Ansible facts are
somewhat extensible so it can include information gathered using the
<a href="https://github.com/opscode/ohai">Ohai</a> or <a href="https://github.com/puppetlabs/facter">Facter</a> tools.</p>

<p>The facts relevant to our example are under the <code>ansible_eth1</code>
key and they include an IPv4 address - which will come in handy in a
moment.</p>

<h2>Facts &amp; templates</h2>

<p>Now let&#8217;s create a <a href="http://docs.ansible.com/template_module.html">template</a> for the haproxy configuration (in
<code>templates/haproxy.cfg.j2</code>):</p>

<p><figure class='code'><figcaption><span>Haproxy config  (haproxy.cfg.j2)</span> <a href='/downloads/code/haproxy.cfg.j2'>download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='jinja'><span class='line'><span class="x">backend web-backend</span>
</span><span class='line'><span class="x">  balance roundrobin</span>
</span><span class='line'><span class="x">  mode http</span>
</span><span class='line'><span class="x">  </span><span class="cp">{%</span> <span class="k">for</span> <span class="nv">host</span> <span class="k">in</span> <span class="nv">groups</span><span class="o">[</span><span class="s1">&#39;webservers&#39;</span><span class="o">]</span> <span class="cp">%}</span><span class="x"></span>
</span><span class='line'><span class="x">     server </span><span class="cp">{{</span> <span class="nv">host</span> <span class="cp">}}</span><span class="x"> </span><span class="cp">{{</span> <span class="nv">hostvars</span><span class="o">[</span><span class="nv">host</span><span class="o">]</span><span class="nv">.ansible_eth1.ipv4.address</span> <span class="cp">}}</span><span class="x">:80 check</span>
</span><span class='line'><span class="x">  </span><span class="cp">{%</span> <span class="k">endfor</span> <span class="cp">%}</span><span class="x"></span>
</span></code></pre></td></tr></table></div></figure></p>

<p>We&#8217;ll also need to ensure that template gets used in the loadbalancer
playbook:</p>

<pre><code class="yaml">---

- hosts: all
  tasks:
    - apt: name=haproxy state=present
    - service: name=haproxy state=started
    - name: Configure haproxy
      template: src=templates/haproxy.cfg.j2 dest=/etc/haproxy/haproxy.cfg
</code></pre>

<p>If we run this now, we&#8217;ll get a cryptic error:</p>

<p><code>fatal: [ariadne] =&gt; {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'webservers'", 'failed': True}</code></p>

<p>One possible reason for this is that we haven&#8217;t defined any groups for
our vagrant instances, let&#8217;s do that now. We&#8217;ll start by defining the
groups at the top of the Vagrantfile, before anything else (but after
the emacs/vi mode comments):</p>

<pre><code class="ruby">groups = {
  "webservers" =&gt; ["minos", "pasiphae"],
  "loadbalancers" =&gt; ["ariadne"],
  "all_groups:children" =&gt; ["webservers", "loadbalancers"]
}
</code></pre>

<p>This correlates to the playbooks we&#8217;ve assigned for each node in the
Vagrantfile. Then we need to refer to that variable in each of our
machine definitions, adding a line that says <code>ansible.groups =
groups</code>, so the modified ariadne definition should now be:</p>

<pre><code class="ruby">  config.vm.define "ariadne" do |ariadne|
    ariadne.vm.provision "ansible" do |ansible|
      ansible.playbook = "loadbalancer.yml"
      ansible.sudo = true
      ansible.groups = groups
    end
  end
</code></pre>

<p>If we run <code>vagrant provision</code> now we get a different error! Ah Ha!
Progress:</p>

<p><code>fatal: [ariadne] =&gt; {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'ansible_eth1'", 'failed': True}</code></p>

<h2>Oh no!</h2>

<p>It would be useful at this point to examine what we <em>do</em> have in that
dictionary object. Maybe I mistyped the key? In order to do that, we
can add a debug line above the haproxy configuration line in the
<code>loadbalancer.yml</code> file, like this: <code>- debug: var=hostvars['minos']</code>.</p>

<p>When we run <code>vagrant provision</code> now, we will get the facts about minos
printed in JSON to the console. It&#8217;ll look something like this:</p>

<pre><code class="json">{
  "hostvars['minos']": {
    "inventory_hostname_short": "minos",
    "inventory_hostname": "minos",
    "group_names": [
      "all_groups",
      "webservers"
    ],
    "ansible_ssh_port": 2200,
    "ansible_ssh_host": "127.0.0.1"
  }
}
</code></pre>

<p>Clearly all those facts gathered aren&#8217;t here. Why? The reason for this
is that Vagrant runs provisioning separately on each virtual machine -
so each ansible run is not aware of anything from another ansible
run. If you look this up online, you will find apparent answers to
this problem that reconfigure vagrant to connect to all hosts when
doing an ansible run. Let&#8217;s do that now.</p>

<p>For each ansible block in the Vagrantfile, add the line:
<code>ansible.limit = 'all'</code>. Let&#8217;s try <code>vagrant provision</code> again now
that&#8217;s in place.</p>

<p>The error that I get after making this change is that SSH is failing.
If we add <code>ansible.verbose = 'vvvv'</code> to each ansible block in the
Vagrantfile then with a lot of scrolling around we can deduce that
ansible is attempting to connect to each machine in the inventory
using the same private key as it would for the machine it&#8217;s currently
provisioning. In other words, while provisioning <code>ariadne</code>, it uses
the <code>ariadne</code> private SSH key to log on to both the other servers.
This won&#8217;t work of course because those SSH keys are generated by
Vagrant per machine. Not only that but the private key is on the host
machine, not on the guests so it&#8217;s a fools errand.</p>

<p>I&#8217;m not sure what kind of SSH key setup would allow <code>ansible.limit =
'all'</code> to work at all, but it&#8217;s hardly straightforward.</p>

<h2>Potential workaround: Redis</h2>

<p>The only way I&#8217;ve discovered to have ansible and Vagrant work well
together is to use <a href="http://docs.ansible.com/playbooks_variables.html#fact-caching">Fact Caching</a>. This allows ansible to cache all
facts from a node in Redis (or memcached) so that nodes can refer to
each other without requiring an extra ssh connection for every node.</p>

<p>In order to enable fact caching, you will need Redis installed and
running. Then create an <code>ansible.cfg</code> file in the same directory as
your Vagrantfile, with the following contents:</p>

<pre><code class="cfg">[defaults]
fact_caching = redis
fact_caching_timeout = 86400
</code></pre>

<p>You will need to provision <code>minos</code> and <code>pasiphae</code> first so that their
facts are stored in Redis before provisioning <code>ariadne</code> (because it
refers to those other nodes):</p>

<pre><code class="bash">vagrant provision minos
vagrant provision pasiphae
</code></pre>

<p>Now that those facts have been gathered, we can run <code>vagrant
provision</code> and it should complete without trouble this time.</p>

<p>Now to verify that the haproxy config has been written as we expect,
we can run <code>vagrant ssh ariadne -- cat /etc/haproxy/haproxy.cfg</code> and
get something akin to:</p>

<pre><code class="text">backend web-backend
  balance roundrobin
  mode http
    server minos 172.28.128.4:80 check
    server pasiphae 172.28.128.5:80 check
</code></pre>

<p>It worked! So although fact caching is intended for use in large
organisations with thousands of nodes (possibly in disparate data
centres) to speed up deployment, it can be handy working around
weaknesses in the vagrant+ansible combination.</p>
]]></content>
  </entry>
  
</feed>
